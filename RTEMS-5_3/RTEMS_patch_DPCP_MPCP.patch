diff --git a/cpukit/Makefile.am b/cpukit/Makefile.am
index 202cf3c346..52ddde5b97 100644
--- a/cpukit/Makefile.am
+++ b/cpukit/Makefile.am
@@ -776,6 +776,7 @@ librtemscpu_a_SOURCES += rtems/src/semflush.c
 librtemscpu_a_SOURCES += rtems/src/semident.c
 librtemscpu_a_SOURCES += rtems/src/semobtain.c
 librtemscpu_a_SOURCES += rtems/src/semrelease.c
+librtemscpu_a_SOURCES += rtems/src/semsetprocessor.c
 librtemscpu_a_SOURCES += rtems/src/semsetpriority.c
 librtemscpu_a_SOURCES += rtems/src/signalcatch.c
 librtemscpu_a_SOURCES += rtems/src/signalsend.c
diff --git a/cpukit/headers.am b/cpukit/headers.am
index 6bd0535a74..dddfe13d68 100644
--- a/cpukit/headers.am
+++ b/cpukit/headers.am
@@ -342,6 +342,8 @@ include_rtems_score_HEADERS += include/rtems/score/corerwlockimpl.h
 include_rtems_score_HEADERS += include/rtems/score/coresem.h
 include_rtems_score_HEADERS += include/rtems/score/coresemimpl.h
 include_rtems_score_HEADERS += include/rtems/score/cpustdatomic.h
+include_rtems_score_HEADERS += include/rtems/score/dpcp.h
+include_rtems_score_HEADERS += include/rtems/score/dpcpimpl.h
 include_rtems_score_HEADERS += include/rtems/score/freechain.h
 include_rtems_score_HEADERS += include/rtems/score/heap.h
 include_rtems_score_HEADERS += include/rtems/score/heapimpl.h
@@ -354,6 +356,8 @@ include_rtems_score_HEADERS += include/rtems/score/isrlock.h
 include_rtems_score_HEADERS += include/rtems/score/memory.h
 include_rtems_score_HEADERS += include/rtems/score/mpci.h
 include_rtems_score_HEADERS += include/rtems/score/mpciimpl.h
+include_rtems_score_HEADERS += include/rtems/score/mpcp.h
+include_rtems_score_HEADERS += include/rtems/score/mpcpimpl.h
 include_rtems_score_HEADERS += include/rtems/score/mppkt.h
 include_rtems_score_HEADERS += include/rtems/score/mrsp.h
 include_rtems_score_HEADERS += include/rtems/score/mrspimpl.h
diff --git a/cpukit/include/rtems/rtems/attr.h b/cpukit/include/rtems/rtems/attr.h
index abc9da08cd..084194a4ba 100644
--- a/cpukit/include/rtems/rtems/attr.h
+++ b/cpukit/include/rtems/rtems/attr.h
@@ -150,6 +150,34 @@ typedef uint32_t   rtems_attribute;
  */
 #define RTEMS_MULTIPROCESSOR_RESOURCE_SHARING 0x00000100
 
+/**
+ *  This attribute constant indicates that the Classic API Semaphore instance
+ *  created will use the Distributed Priority Ceiling Protocol.
+ *
+ *  @note The semaphore instance must be a binary semaphore.
+ */
+#define RTEMS_DISTRIBUTED_PRIORITY_CEILING 0x00004000
+
+/**
+ *  This attribute constant indicates that the Classic API Semaphore instance
+ *  created will NOT use the Distributed Priority Ceiling Protocol.
+ */
+#define RTEMS_NO_DISTRIBUTED_PRIORITY_CEILING 0x00000000
+
+/**
+ *  This attribute constant indicates that the Classic API Semaphore instance
+ *  created will NOT use the Multiprocessor Priority Ceiling Protocol.
+ */
+#define RTEMS_NO_MULTIPROCESSOR_PRIORITY_CEILING 0x00000000
+
+/**
+ *  This attribute constant indicates that the Classic API Semaphore instance
+ *  created will use the Multiprocessor Priority Ceiling Protocol.
+ *
+ *  @note The semaphore instance must be a binary semaphore.
+ */
+#define RTEMS_MULTIPROCESSOR_PRIORITY_CEILING 0x00002000
+
 /******************** RTEMS Barrier Specific Attributes ********************/
 
 /**
diff --git a/cpukit/include/rtems/rtems/sem.h b/cpukit/include/rtems/rtems/sem.h
index 3dcdfbfda8..b4d9b69b9d 100644
--- a/cpukit/include/rtems/rtems/sem.h
+++ b/cpukit/include/rtems/rtems/sem.h
@@ -119,6 +119,25 @@ rtems_status_code rtems_semaphore_obtain(
   rtems_interval timeout
 );
 
+/**
+ * @brief RTEMS Semaphore Set Processor
+ *
+ * Sets the synchronization processor for the DPCP semaphores. It
+ * attempts to obtain a unit from the semaphore associated with ID.
+ *
+ *
+ * @param[in] id is the semaphore id
+ * @param[in] cpu is the number of the processor in the system
+ *
+ * @retval This method returns RTEMS_SUCCESSFUL if there was not an
+ *         error. Otherwise, a status code is returned indicating the
+ *         source of the error.
+ */
+rtems_status_code rtems_semaphore_set_processor(
+  rtems_id id,
+  int      cpu
+);
+
 /**
  *  @brief RTEMS Semaphore Release
  *
diff --git a/cpukit/include/rtems/rtems/semdata.h b/cpukit/include/rtems/rtems/semdata.h
index 497f786dfb..b959286a8e 100644
--- a/cpukit/include/rtems/rtems/semdata.h
+++ b/cpukit/include/rtems/rtems/semdata.h
@@ -21,6 +21,8 @@
 #include <rtems/rtems/sem.h>
 #include <rtems/score/coremutex.h>
 #include <rtems/score/coresem.h>
+#include <rtems/score/dpcp.h>
+#include <rtems/score/mpcp.h>
 #include <rtems/score/mrsp.h>
 #include <rtems/score/object.h>
 
@@ -78,6 +80,8 @@ typedef struct {
 
 #if defined(RTEMS_SMP)
     MRSP_Control MRSP;
+    DPCP_Control DPCP;
+    MPCP_Control MPCP;
 #endif
   } Core_control;
 }   Semaphore_Control;
diff --git a/cpukit/include/rtems/rtems/semimpl.h b/cpukit/include/rtems/rtems/semimpl.h
index 0cb78e35d4..9a1def6245 100644
--- a/cpukit/include/rtems/rtems/semimpl.h
+++ b/cpukit/include/rtems/rtems/semimpl.h
@@ -20,6 +20,8 @@
 #include <rtems/rtems/semdata.h>
 #include <rtems/score/coremuteximpl.h>
 #include <rtems/score/coresemimpl.h>
+#include <rtems/score/dpcpimpl.h>
+#include <rtems/score/mpcpimpl.h>
 #include <rtems/score/mrspimpl.h>
 
 #ifdef __cplusplus
@@ -47,7 +49,9 @@ typedef enum {
   SEMAPHORE_VARIANT_COUNTING
 #if defined(RTEMS_SMP)
   ,
-  SEMAPHORE_VARIANT_MRSP
+  SEMAPHORE_VARIANT_MRSP,
+  SEMAPHORE_VARIANT_DPCP,
+  SEMAPHORE_VARIANT_MPCP
 #endif
 } Semaphore_Variant;
 
diff --git a/cpukit/include/rtems/score/dpcp.h b/cpukit/include/rtems/score/dpcp.h
new file mode 100644
index 0000000000..dc7161240d
--- /dev/null
+++ b/cpukit/include/rtems/score/dpcp.h
@@ -0,0 +1,54 @@
+/*
+ * Copyright (c) 2018 Jan Pham.  All rights reserved.
+ *
+ * The license and distribution terms for this file may be
+ * found in the file LICENSE in this distribution or at
+ * http://www.rtems.org/license/LICENSE.
+ */
+
+#ifndef _RTEMS_SCORE_DPCP
+#define _RTEMS_SCORE_DPCP
+
+#include <rtems/score/cpuopts.h>
+
+#if defined(RTEMS_SMP)
+
+#include <rtems/score/threadq.h>
+#include <rtems/score/percpu.h>
+
+#ifdef __cplusplus
+extern "C" {
+#endif /* __cplusplus */
+
+/**
+ * @brief DPCP control block.
+ */
+typedef struct {
+  /**
+   * @brief The thread queue to manage ownership and waiting threads.
+   */
+  Thread_queue_Control Wait_queue;
+
+  /**
+  * @brief User-defined sychronization cpu, where the thread migrates to
+  */
+  Per_CPU_Control *cpu;
+
+
+  /**
+   * @brief The ceiling priority used by the owner thread.
+   */
+  Priority_Node Ceiling_priority;
+
+
+} DPCP_Control;
+
+/** @} */
+
+#ifdef __cplusplus
+}
+#endif /* __cplusplus */
+
+#endif /* RTEMS_SMP */
+
+#endif /* _RTEMS_SCORE_DPCP */
diff --git a/cpukit/include/rtems/score/dpcpimpl.h b/cpukit/include/rtems/score/dpcpimpl.h
new file mode 100644
index 0000000000..f2d8dcef2a
--- /dev/null
+++ b/cpukit/include/rtems/score/dpcpimpl.h
@@ -0,0 +1,428 @@
+#ifndef _RTEMS_SCORE_DPCPIMPL_H
+#define _RTEMS_SCORE_DPCPIMPL_H
+
+#include <rtems/score/dpcp.h>
+
+#if defined(RTEMS_SMP)
+
+#include <rtems/score/assert.h>
+#include <rtems/score/status.h>
+#include <rtems/score/threadqimpl.h>
+#include <rtems/score/watchdogimpl.h>
+#include <rtems/score/wkspace.h>
+#include <rtems/score/scheduler.h>
+
+#ifdef __cplusplus
+extern "C" {
+#endif /* __cplusplus */
+
+/**
+ * @addtogroup ScoreDPCP
+ *
+ * @{
+ */
+#define DPCP_TQ_OPERATIONS &_Thread_queue_Operations_priority
+
+/**
+ * @brief Migrates Thread to an synchronization processor.
+ *
+ * @param executing The executing Thread.
+ * @param dpcp The semaphore control block.
+ */
+RTEMS_INLINE_ROUTINE void _DPCP_Migrate(
+  Thread_Control *executing,
+  DPCP_Control   *dpcp
+)
+{
+  _Scheduler_Migrate_To( executing, dpcp->cpu, &( dpcp->Ceiling_priority ) );
+}
+
+/**
+ * @brief Migrates the task back to the application processor.
+ *
+ * @param executing The executing Thread.
+ * @param dpcp The semaphore control block.
+ */
+RTEMS_INLINE_ROUTINE void _DPCP_Migrate_Back(
+  Thread_Control *executing,
+  DPCP_Control   *dpcp
+)
+{
+  _Scheduler_Migrate_Back( executing,dpcp->cpu );
+}
+
+/**
+ * @brief Acquires critical according to DPCP.
+ *
+ * @param dpcp The DPCP control for the operation.
+ * @param queue_context The thread queue context.
+ */
+RTEMS_INLINE_ROUTINE void _DPCP_Acquire_critical(
+  DPCP_Control         *dpcp,
+  Thread_queue_Context *queue_context
+)
+{
+  _Thread_queue_Acquire_critical( &dpcp->Wait_queue, queue_context );
+}
+
+/**
+ * @brief Releases according to DPCP.
+ *
+ * @param dpcp The DPCP control for the operation.
+ * @param queue_context The thread queue context.
+ */
+RTEMS_INLINE_ROUTINE void _DPCP_Release(
+  DPCP_Control         *dpcp,
+  Thread_queue_Context *queue_context
+)
+{
+  _Thread_queue_Release( &dpcp->Wait_queue, queue_context );
+}
+
+/**
+ * @brief Gets owner of the DPCP control.
+ *
+ * @param dpcp The DPCP control to get the owner from.
+ *
+ * @return The owner of the dpcp control.
+ */
+RTEMS_INLINE_ROUTINE Thread_Control *_DPCP_Get_owner(
+  const DPCP_Control *dpcp
+)
+{
+  return dpcp->Wait_queue.Queue.owner;
+}
+
+/**
+ * @brief Sets owner of the DPCP control.
+ *
+ * @param[out] dpcp The DPCP control to set the owner of.
+ * @param owner The desired new owner for @a dpcp.
+ */
+RTEMS_INLINE_ROUTINE void _DPCP_Set_owner(
+  DPCP_Control   *dpcp,
+  Thread_Control *owner
+)
+{
+  dpcp->Wait_queue.Queue.owner = owner;
+}
+
+/**
+ * @brief Gets ceiling priority of the DPCP control.
+ *
+ * @param dpcp The dpcp to get the priority from.
+ *
+ * @return The priority of the DPCP control.
+ */
+RTEMS_INLINE_ROUTINE Priority_Control _DPCP_Get_priority(
+  const DPCP_Control *dpcp
+)
+{
+  return dpcp->Ceiling_priority.priority;
+}
+
+
+/**
+ * @brief Sets the ceiling priority of the DPCP control
+ *
+ * @param[out] dpcp The DPCP control to set the priority of.
+ * @param priority_ceiling The new priority for the DPCP Control
+ * @param queue_context The Thread queue context
+ */
+RTEMS_INLINE_ROUTINE void _DPCP_Set_priority(
+  DPCP_Control         *dpcp,
+  Priority_Control      priority_ceiling,
+  Thread_queue_Context *queue_context
+)
+{
+  Thread_Control *owner;
+  owner = _DPCP_Get_owner( dpcp );
+  if ( owner != NULL ) {
+   //Do nothing, thread executing right now
+  } else {
+    dpcp->Ceiling_priority.priority = priority_ceiling;
+  }
+}
+
+/**
+ * @brief Gets the synchronization CPU of the DPCP Control, where the task migrates to.
+ *
+ * @retval The Per_CPU_Control control block
+ */
+RTEMS_INLINE_ROUTINE Per_CPU_Control *_DPCP_Get_CPU(
+  DPCP_Control *dpcp
+)
+{
+  return dpcp->cpu;
+}
+
+/**
+ * @brief Sets the synchronization CPU of the DPCP Control.
+ *
+ * @param dpcp The semaphore control block
+ * @param cpu The synchronization processor it changes to.
+ * @param queue_context struct to secure sempahore access
+ */
+RTEMS_INLINE_ROUTINE void _DPCP_Set_CPU(
+  DPCP_Control         *dpcp,
+  Per_CPU_Control      *cpu,
+  Thread_queue_Context *queue_context
+)
+{
+  _DPCP_Acquire_critical( dpcp, queue_context );
+  dpcp->cpu = cpu;
+  _DPCP_Release( dpcp, queue_context );
+}
+
+/**
+ * @brief Claims ownership of the DPCP control.
+ *
+ * @param dpcp The DPCP control to claim the ownership of.
+ * @param[in, out] executing The currently executing thread.
+ * @param queue_context The thread queue context.
+ *
+ * @retval STATUS_SUCCESSFUL The operation succeeded.
+ * @retval STATUS_MUTEX_CEILING_VIOLATED The wait priority of the executing
+ *      thread exceeds the ceiling priority.
+ */
+RTEMS_INLINE_ROUTINE Status_Control _DPCP_Claim_ownership(
+  DPCP_Control         *dpcp,
+  Thread_Control       *executing,
+  Thread_queue_Context *queue_context
+)
+{
+  ISR_lock_Context  lock_context;
+  Scheduler_Node   *scheduler_node;
+  Per_CPU_Control  *cpu_self;
+
+  _Thread_Wait_acquire_default_critical( executing, &lock_context );
+  scheduler_node = _Thread_Scheduler_get_home_node( executing );
+
+  if ( _Priority_Get_priority( &scheduler_node->Wait.Priority ) <
+      dpcp->Ceiling_priority.priority ) {
+      _Thread_Wait_release_default_critical( executing, &lock_context );
+      _DPCP_Release( dpcp, queue_context );
+      return STATUS_MUTEX_CEILING_VIOLATED;
+  }
+
+  _DPCP_Set_owner( dpcp, executing );
+  cpu_self = _Thread_queue_Dispatch_disable( queue_context );
+  _DPCP_Release( dpcp, queue_context );
+  _DPCP_Migrate( executing, dpcp );
+  _Thread_Wait_release_default_critical( executing, &lock_context );
+  _Thread_Dispatch_enable( cpu_self );
+  return STATUS_SUCCESSFUL;
+}
+
+/**
+ * @brief Initializes a DPCP control.
+ *
+ * @param[out] dpcp The DPCP control that is initialized.
+ * @param scheduler The scheduler for the operation.
+ * @param ceiling_priority
+ * @param executing The currently executing thread.  Ignored in this method.
+ * @param initially_locked Indicates whether the DPCP control shall be initally
+ *      locked. If it is initially locked, this method returns STATUS_INVALID_NUMBER.
+ *
+ * @retval STATUS_SUCCESSFUL The operation succeeded.
+ * @retval STATUS_INVALID_NUMBER The DPCP control is initially locked.
+ */
+RTEMS_INLINE_ROUTINE Status_Control _DPCP_Initialize(
+  DPCP_Control            *dpcp,
+  const Scheduler_Control *scheduler,
+  Priority_Control         ceiling_priority,
+  Thread_Control          *executing,
+  bool                     initially_locked
+)
+{
+  if ( initially_locked ) {
+    return STATUS_INVALID_NUMBER;
+  }
+
+  dpcp->cpu = _Per_CPU_Get_by_index( 1 );
+  _Priority_Node_initialize( &dpcp->Ceiling_priority, ceiling_priority );
+  _Thread_queue_Object_initialize( &dpcp->Wait_queue );
+  return STATUS_SUCCESSFUL;
+}
+/**
+ * @brief Waits for the ownership of the DPCP control.
+ *
+ * @param dpcp The DPCP control to get the ownership of.
+ * @param executing The currently executing thread.
+ * @param queue_context the thread queue context.
+ *
+ * @retval STATUS_SUCCESSFUL The operation succeeded.
+ * @retval STATUS_DEADLOCK A deadlock occured.
+ * @retval STATUS_TIMEOUT A timeout occured.
+ */
+RTEMS_INLINE_ROUTINE Status_Control _DPCP_Wait_for_ownership(
+  DPCP_Control         *dpcp,
+  Thread_Control       *executing,
+  Thread_queue_Context *queue_context
+)
+{
+  _Thread_queue_Context_set_thread_state(
+    queue_context,
+    STATES_WAITING_FOR_MUTEX
+  );
+  _Thread_queue_Context_set_deadlock_callout(
+    queue_context,
+    _Thread_queue_Deadlock_status
+  );
+  _Thread_queue_Enqueue(
+    &dpcp->Wait_queue.Queue,
+    DPCP_TQ_OPERATIONS,
+    executing,
+    queue_context
+  );
+  return STATUS_SUCCESSFUL;
+}
+
+/**
+ * @brief Seizes the DPCP control.
+ *
+ * @param[in, out] dpcp The DPCP control to seize the control of.
+ * @param[in, out] executing The currently executing thread.
+ * @param wait Indicates whether the calling thread is willing to wait.
+ * @param queue_context The thread queue context.
+ *
+ * @retval STATUS_SUCCESSFUL The operation succeeded.
+ * @retval STATUS_MUTEX_CEILING_VIOLATED The wait priority of the executing
+ *      thread exceeds the ceiling priority.
+ * @retval STATUS_UNAVAILABLE The executing thread is already the owner of
+ *      the DPCP control.  Seizing it is not possible.
+ */
+RTEMS_INLINE_ROUTINE Status_Control _DPCP_Seize(
+  DPCP_Control         *dpcp,
+  Thread_Control       *executing,
+  bool                  wait,
+  Thread_queue_Context *queue_context
+)
+{
+  Status_Control  status;
+  Thread_Control *owner;
+
+  _DPCP_Acquire_critical( dpcp, queue_context );
+
+  owner = _DPCP_Get_owner( dpcp );
+
+  if ( owner == NULL ) {
+    status = _DPCP_Claim_ownership( dpcp, executing, queue_context );
+  } else if ( owner == executing ) {
+    _DPCP_Release( dpcp, queue_context );
+    status = STATUS_UNAVAILABLE;
+  } else if ( wait ) {
+    status = _DPCP_Wait_for_ownership( dpcp, executing, queue_context );
+  } else {
+    _DPCP_Release( dpcp, queue_context );
+    status = STATUS_UNAVAILABLE;
+  }
+
+  return status;
+}
+
+/**
+ * @brief Surrenders the DPCP control.
+ *
+ * @param[in, out] dpcp The DPCP control to surrender the control of.
+ * @param[in, out] executing The currently executing thread.
+ * @param queue_context The thread queue context.
+ *
+ * @retval STATUS_SUCCESSFUL The operation succeeded.
+ * @retval STATUS_NOT_OWNER The executing thread does not own the DPCP control.
+ */
+RTEMS_INLINE_ROUTINE Status_Control _DPCP_Surrender(
+  DPCP_Control         *dpcp,
+  Thread_Control       *executing,
+  Thread_queue_Context *queue_context
+)
+{
+  ISR_lock_Context  lock_context;
+  Per_CPU_Control  *cpu_self;
+  Thread_Control   *new_owner;
+
+  _DPCP_Acquire_critical( dpcp, queue_context );
+  cpu_self = _Thread_Dispatch_disable_critical( &queue_context->Lock_context.Lock_context );
+  if (_DPCP_Get_owner( dpcp ) != executing) {
+    _DPCP_Release( dpcp , queue_context );
+    _Thread_Dispatch_enable( cpu_self );
+    return STATUS_NOT_OWNER;
+  }
+
+  _Thread_queue_Context_clear_priority_updates( queue_context );
+  new_owner = _Thread_queue_First_locked(
+                &dpcp->Wait_queue,
+	        DPCP_TQ_OPERATIONS
+	      );
+  _DPCP_Set_owner( dpcp, new_owner );
+
+  if ( new_owner != NULL ) {
+  #if defined(RTEMS_MULTIPROCESSING)
+    if ( _Objects_Is_local_id( new_owner->Object.id ) )
+  #endif
+    {
+    }
+    _Thread_queue_Extract_critical(
+      &dpcp->Wait_queue.Queue,
+      CORE_MUTEX_TQ_OPERATIONS,
+      new_owner,
+      queue_context
+    );
+    _Thread_Wait_acquire_default_critical( new_owner, &lock_context );
+    _DPCP_Migrate(new_owner, dpcp);
+    _Thread_Wait_release_default_critical( new_owner, &lock_context );
+  } else {
+    _DPCP_Release( dpcp, queue_context );
+  }
+
+  _Thread_Wait_acquire_default_critical( executing, &lock_context );
+  _DPCP_Migrate_Back( executing, dpcp );
+  _Thread_Wait_release_default_critical( executing, &lock_context );
+  _Thread_Dispatch_enable( cpu_self );
+
+  return STATUS_SUCCESSFUL;
+}
+
+/**
+ * @brief Checks if the DPCP control can be destroyed.
+ *
+ * @param dpcp The DPCP control for the operation.
+ *
+ * @retval STATUS_SUCCESSFUL The DPCP is currently not used
+ *      and can be destroyed.
+ * @retval STATUS_RESOURCE_IN_USE The DPCP control is in use,
+ *      it cannot be destroyed.
+ */
+RTEMS_INLINE_ROUTINE Status_Control _DPCP_Can_destroy(
+  DPCP_Control *dpcp
+)
+{
+  if ( _DPCP_Get_owner( dpcp ) != NULL ) {
+    return STATUS_RESOURCE_IN_USE;
+  }
+  return STATUS_SUCCESSFUL;
+}
+
+/**
+ * @brief Destroys the DPCP control
+ *
+ * @param[in, out] The dpcp that is about to be destroyed.
+ * @param queue_context The thread queue context.
+ */
+RTEMS_INLINE_ROUTINE void _DPCP_Destroy(
+  DPCP_Control         *dpcp,
+  Thread_queue_Context *queue_context
+)
+{
+  _DPCP_Release( dpcp, queue_context );
+  _Thread_queue_Destroy( &dpcp->Wait_queue );
+}
+/** @} */
+
+#ifdef __cplusplus
+}
+#endif /* __cplusplus */
+
+#endif /* RTEMS_SMP */
+
+#endif /* _RTEMS_SCORE_DPCPIMPL_H */
\ No newline at end of file
diff --git a/cpukit/include/rtems/score/mpcp.h b/cpukit/include/rtems/score/mpcp.h
new file mode 100644
index 0000000000..e7ac01cf52
--- /dev/null
+++ b/cpukit/include/rtems/score/mpcp.h
@@ -0,0 +1,51 @@
+#ifndef SOURCE_MPCP_H
+#define SOURCE_MPCP_H
+
+#if defined(RTEMS_SMP)
+
+#include <rtems/score/threadq.h>
+
+#ifdef __cplusplus
+extern "C" {
+#endif /* __cplusplus */
+
+/**
+ * @defgroup ScoreMPCP Multiprocessor Resource Sharing Protocol Handler
+ *
+ * @ingroup Score
+ *
+ * @brief Multiprocessor Resource Sharing Protocol (MPCP).
+ *
+ *
+ * @{
+ */
+
+/**
+ * @brief MPCP control block.
+ */
+typedef struct {
+  /**
+   * @brief The thread queue to manage ownership and waiting threads.
+   */
+  Thread_queue_Control Wait_queue;
+
+  /**
+   * @brief The ceiling priority used by the owner thread.
+   */
+  Priority_Node Ceiling_priority;
+
+  /**
+   * @brief One ceiling priority per scheduler instance.
+   */
+  Priority_Control *ceiling_priorities;
+} MPCP_Control;
+
+/** @} */
+
+#ifdef __cplusplus
+}
+#endif /* __cplusplus */
+
+#endif /* RTEMS_SMP */
+
+#endif //SOURCE_MPCP_H
\ No newline at end of file
diff --git a/cpukit/include/rtems/score/mpcpimpl.h b/cpukit/include/rtems/score/mpcpimpl.h
new file mode 100644
index 0000000000..298d474bc8
--- /dev/null
+++ b/cpukit/include/rtems/score/mpcpimpl.h
@@ -0,0 +1,520 @@
+/*
+ * Copyright (c) 2014, 2016 embedded brains GmbH.  All rights reserved.
+ *
+ *  embedded brains GmbH
+ *  Dornierstr. 4
+ *  82178 Puchheim
+ *  Germany
+ *  <rtems@embedded-brains.de>
+ *
+ * The license and distribution terms for this file may be
+ * found in the file LICENSE in this distribution or at
+ * http://www.rtems.org/license/LICENSE.
+ */
+
+#ifndef _RTEMS_SCORE_MPCPIMPL_H
+#define _RTEMS_SCORE_MPCPIMPL_H
+
+
+#include <rtems/score/mpcp.h>
+
+#if defined(RTEMS_SMP)
+
+#include <rtems/score/assert.h>
+#include <rtems/score/status.h>
+#include <rtems/score/threadqimpl.h>
+#include <rtems/score/watchdogimpl.h>
+#include <rtems/score/wkspace.h>
+#include <rtems/score/schedulerimpl.h>
+#include <rtems/score/threadimpl.h>
+
+#ifdef __cplusplus
+extern "C" {
+#endif /* __cplusplus */
+
+/**
+ * @addtogroup ScoreMPCP
+ *
+ * @{
+ */
+
+#define MPCP_TQ_OPERATIONS &_Thread_queue_Operations_priority
+
+/**
+ * @brief Acquires critical according to MPCP.
+ *
+ * @param mpcp The MPCP control for the operation.
+ * @param queue_context The thread queue context.
+ */
+RTEMS_INLINE_ROUTINE void _MPCP_Acquire_critical(
+        MPCP_Control         *mpcp,
+        Thread_queue_Context *queue_context
+)
+{
+    _Thread_queue_Acquire_critical( &mpcp->Wait_queue, queue_context );
+}
+
+/**
+ * @brief Releases according to MPCP.
+ *
+ * @param mpcp The MPCP control for the operation.
+ * @param queue_context The thread queue context.
+ */
+RTEMS_INLINE_ROUTINE void _MPCP_Release(
+  MPCP_Control         *mpcp,
+  Thread_queue_Context *queue_context
+)
+{
+    _Thread_queue_Release( &mpcp->Wait_queue, queue_context );
+}
+
+/**
+ * @brief Gets owner of the MPCP control.
+ *
+ * @param mpcp The MPCP control to get the owner from.
+ *
+ * @return The owner of the MPCP control.
+ */
+RTEMS_INLINE_ROUTINE Thread_Control *_MPCP_Get_owner(
+        const MPCP_Control *mpcp
+)
+{
+    return mpcp->Wait_queue.Queue.owner;
+}
+
+/**
+ * @brief Sets owner of the MPCP control.
+ *
+ * @param[out] mpcp The MPCP control to set the owner of.
+ * @param owner The desired new owner for @a mpcp.
+ */
+RTEMS_INLINE_ROUTINE void _MPCP_Set_owner(
+        MPCP_Control   *mpcp,
+        Thread_Control *owner
+)
+{
+    mpcp->Wait_queue.Queue.owner = owner;
+}
+
+/**
+ * @brief Gets priority of the MPCP control.
+ *
+ * @param mpcp The mpcp to get the priority from.
+ * @param scheduler The corresponding scheduler.
+ *
+ * @return The priority of the MPCP control.
+ */
+RTEMS_INLINE_ROUTINE Priority_Control _MPCP_Get_priority(
+        const MPCP_Control      *mpcp,
+        const Scheduler_Control *scheduler
+)
+{
+    uint32_t scheduler_index;
+
+    scheduler_index = _Scheduler_Get_index( scheduler );
+    return mpcp ->ceiling_priorities[scheduler_index];
+}
+
+/**
+ * @brief Sets priority of the MPCP control
+ *
+ * @param[out] mpcp The MPCP control to set the priority of.
+ * @param scheduler The corresponding scheduler.
+ * @param new_priority The new priority for the MPCP control
+ */
+RTEMS_INLINE_ROUTINE void _MPCP_Set_priority(
+        MPCP_Control            *mpcp,
+        const Scheduler_Control *scheduler,
+        Priority_Control         new_priority
+)
+{
+    uint32_t scheduler_index;
+
+    scheduler_index = _Scheduler_Get_index( scheduler );
+    mpcp->ceiling_priorities[ scheduler_index ] = new_priority;
+}
+
+/**
+ * @brief Adds the priority to the given thread.
+ *
+ * @param mpcp The MPCP control for the operation.
+ * @param[in, out] thread The thread to add the priority node to.
+ * @param[out] priority_node The priority node to initialize and add to
+ *      the thread.
+ * @param queue_context The thread queue context.
+ *
+ * @retval STATUS_SUCCESSFUL The operation succeeded.
+ * @retval STATUS_MUTEX_CEILING_VIOLATED The wait priority of the thread
+ *      exceeds the ceiling priority.
+ */
+RTEMS_INLINE_ROUTINE Status_Control _MPCP_Raise_priority(
+        MPCP_Control         *mpcp,
+        Thread_Control       *thread,
+        Priority_Node        *priority_node,
+        Thread_queue_Context *queue_context
+)
+{
+    Status_Control           status;
+    ISR_lock_Context         lock_context;
+    const Scheduler_Control *scheduler;
+    Priority_Control         ceiling_priority;
+    Scheduler_Node          *scheduler_node;
+
+    _Thread_queue_Context_clear_priority_updates( queue_context );
+    _Thread_Wait_acquire_default_critical( thread, &lock_context );
+
+    scheduler = _Thread_Scheduler_get_home( thread );
+    scheduler_node = _Thread_Scheduler_get_home_node( thread );
+    ceiling_priority = _MPCP_Get_priority( mpcp, scheduler );
+
+    if (
+            ceiling_priority
+            <= _Priority_Get_priority( &scheduler_node->Wait.Priority )
+            ) {
+        _Priority_Node_initialize( priority_node, ceiling_priority );
+        _Thread_Priority_add( thread, priority_node, queue_context );
+        status = STATUS_SUCCESSFUL;
+    } else {
+        status = STATUS_MUTEX_CEILING_VIOLATED;
+    }
+
+    _Thread_Wait_release_default_critical( thread, &lock_context );
+    return status;
+}
+
+/**
+ * @brief Removes the priority from the given thread.
+ *
+ * @param[in, out] The thread to remove the priority from.
+ * @param priority_node The priority node to remove from the thread
+ * @param queue_context The thread queue context.
+ */
+RTEMS_INLINE_ROUTINE void _MPCP_Remove_priority(
+        Thread_Control       *thread,
+        Priority_Node        *priority_node,
+        Thread_queue_Context *queue_context
+)
+{
+    ISR_lock_Context lock_context;
+
+    _Thread_queue_Context_clear_priority_updates( queue_context );
+    _Thread_Wait_acquire_default_critical( thread, &lock_context );
+    _Thread_Priority_remove( thread, priority_node, queue_context );
+    _Thread_Wait_release_default_critical( thread, &lock_context );
+}
+
+/**
+ * @brief Replaces the given priority node with the ceiling priority of
+ *      the MPCP control.
+ *
+ * @param mpcp The mpcp control for the operation.
+ * @param[out] thread The thread to replace the priorities.
+ * @param ceiling_priority The node to be replaced.
+ */
+RTEMS_INLINE_ROUTINE void _MPCP_Replace_priority(
+        MPCP_Control   *mpcp,
+        Thread_Control *thread,
+        Priority_Node  *ceiling_priority
+)
+{
+    ISR_lock_Context lock_context;
+
+    _Thread_Wait_acquire_default( thread, &lock_context );
+    _Thread_Priority_replace(
+            thread,
+            ceiling_priority,
+            &mpcp->Ceiling_priority
+    );
+    _Thread_Wait_release_default( thread, &lock_context );
+}
+
+/**
+ * @brief Claims ownership of the MPCP control.
+ *
+ * @param mpcp The MPCP control to claim the ownership of.
+ * @param[in, out] executing The currently executing thread.
+ * @param queue_context The thread queue context.
+ *
+ * @retval STATUS_SUCCESSFUL The operation succeeded.
+ * @retval STATUS_MUTEX_CEILING_VIOLATED The wait priority of the executing
+ *      thread exceeds the ceiling priority.
+ */
+RTEMS_INLINE_ROUTINE Status_Control _MPCP_Claim_ownership(
+        MPCP_Control         *mpcp,
+        Thread_Control       *executing,
+        Thread_queue_Context *queue_context
+)
+{
+    Status_Control   status;
+    Per_CPU_Control *cpu_self;
+
+    status = _MPCP_Raise_priority(
+            mpcp,
+            executing,
+            &mpcp->Ceiling_priority,
+            queue_context
+    );
+
+    if ( status != STATUS_SUCCESSFUL ) {
+        _MPCP_Release( mpcp, queue_context );
+        return status;
+    }
+
+    _MPCP_Set_owner( mpcp, executing );
+    cpu_self = _Thread_queue_Dispatch_disable( queue_context );
+    _MPCP_Release( mpcp, queue_context );
+    _Thread_Priority_update( queue_context );
+    _Thread_Dispatch_enable( cpu_self );
+
+    return RTEMS_SUCCESSFUL;
+}
+
+/**
+ * @brief Initializes a MPCP control.
+ *
+ * @param[out] mpcp The MPCP control that is initialized.
+ * @param scheduler The scheduler for the operation.
+ * @param ceiling_priority
+ * @param executing The currently executing thread.  Ignored in this method.
+ * @param initially_locked Indicates whether the MPCP control shall be initally
+ *      locked. If it is initially locked, this method returns STATUS_INVALID_NUMBER.
+ *
+ * @retval STATUS_SUCCESSFUL The operation succeeded.
+ * @retval STATUS_INVALID_NUMBER The MPCP control is initially locked.
+ * @retval STATUS_NO_MEMORY There is not enough memory to allocate.
+ */
+RTEMS_INLINE_ROUTINE Status_Control _MPCP_Initialize(
+        MPCP_Control            *mpcp,
+        const Scheduler_Control *scheduler,
+        Priority_Control         ceiling_priority,
+        Thread_Control          *executing,
+        bool                     initially_locked
+)
+{
+  (void) executing;
+    uint64_t scheduler_count = _Scheduler_Count;
+    uint32_t i;
+
+    if ( initially_locked ) {
+        return STATUS_INVALID_NUMBER;
+    }
+
+    mpcp->ceiling_priorities = (Priority_Control *)_Workspace_Allocate(
+            sizeof( *mpcp->ceiling_priorities ) * scheduler_count
+    );
+    if ( mpcp->ceiling_priorities == NULL ) {
+        return STATUS_NO_MEMORY;
+    }
+
+    for ( i = 0 ; i < scheduler_count ; ++i ) {
+        const Scheduler_Control *scheduler_of_index;
+
+        scheduler_of_index = &_Scheduler_Table[ i ];
+
+        if ( scheduler != scheduler_of_index ) {
+            mpcp->ceiling_priorities[ i ] =
+                   _Scheduler_Map_priority( scheduler_of_index, 1);
+        } else {
+            mpcp->ceiling_priorities[ i ] = ceiling_priority;
+        }
+    }
+
+    _Thread_queue_Object_initialize( &mpcp->Wait_queue );
+    return STATUS_SUCCESSFUL;
+}
+
+/**
+ * @brief Waits for the ownership of the MPCP control.
+ *
+ * @param[in, out] mpcp The MPCP control to get the ownership of.
+ * @param[in, out] executing The currently executing thread.
+ * @param queue_context the thread queue context.
+ *
+ * @retval STATUS_SUCCESSFUL The operation succeeded.
+ * @retval STATUS_MUTEX_CEILING_VIOLATED The wait priority of the
+ *      currently executing thread exceeds the ceiling priority.
+ * @retval STATUS_DEADLOCK A deadlock occured.
+ * @retval STATUS_TIMEOUT A timeout occured.
+ */
+RTEMS_INLINE_ROUTINE Status_Control _MPCP_Wait_for_ownership(
+  MPCP_Control         *mpcp,
+  Thread_Control       *executing,
+  Thread_queue_Context *queue_context
+)
+{
+
+  Status_Control status;
+  Priority_Node  ceiling_priority;
+
+  _Thread_queue_Context_set_thread_state(
+    queue_context,
+    STATES_WAITING_FOR_SEMAPHORE
+  );
+
+  _Thread_queue_Context_set_deadlock_callout(
+    queue_context,
+    _Thread_queue_Deadlock_status
+  );
+
+  _Thread_queue_Enqueue(
+    &mpcp->Wait_queue.Queue,
+    MPCP_TQ_OPERATIONS,
+    executing,
+    queue_context
+  );
+
+  status = _MPCP_Raise_priority(
+    mpcp,
+    executing,
+    &ceiling_priority,
+    queue_context
+  );
+
+  if ( status != STATUS_SUCCESSFUL ) {
+    _MPCP_Release( mpcp, queue_context );
+    return status;
+  }
+  _MPCP_Replace_priority( mpcp, executing, &ceiling_priority );
+
+  return status;
+}
+
+/**
+ * @brief Seizes the MPCP control.
+ *
+ * @param[in, out] mpcp The MPCP control to seize the control of.
+ * @param[in, out] executing The currently executing thread.
+ * @param wait Indicates whether the calling thread is willing to wait.
+ * @param queue_context The thread queue context.
+ *
+ * @retval STATUS_SUCCESSFUL The operation succeeded.
+ * @retval STATUS_MUTEX_CEILING_VIOLATED The wait priority of the executing
+ *      thread exceeds the ceiling priority.
+ * @retval STATUS_UNAVAILABLE The executing thread is already the owner of
+ *      the MPCP control.  Seizing it is not possible.
+ */
+RTEMS_INLINE_ROUTINE Status_Control _MPCP_Seize(
+        MPCP_Control         *mpcp,
+        Thread_Control       *executing,
+        bool                  wait,
+        Thread_queue_Context *queue_context
+)
+{
+    Status_Control  status;
+    Thread_Control *owner;
+
+    _MPCP_Acquire_critical( mpcp, queue_context );
+
+    owner = _MPCP_Get_owner( mpcp );
+
+    if ( owner == NULL ) {
+        status = _MPCP_Claim_ownership( mpcp, executing, queue_context );
+    } else if ( owner == executing ) {
+        _MPCP_Release( mpcp, queue_context );
+        status = STATUS_UNAVAILABLE;
+    } else if ( wait ) {
+        status = _MPCP_Wait_for_ownership( mpcp, executing, queue_context );
+    } else {
+        _MPCP_Release( mpcp, queue_context );
+        status = STATUS_UNAVAILABLE;
+    }
+
+    return status;
+}
+
+/**
+ * @brief Surrenders the MPCP control.
+ *
+ * @param[in, out] mpcp The MPCP control to surrender the control of.
+ * @param[in, out] executing The currently executing thread.
+ * @param queue_context The thread queue context.
+ *
+ * @retval STATUS_SUCCESSFUL The operation succeeded.
+ * @retval STATUS_NOT_OWNER The executing thread does not own the MPCP control.
+ */
+RTEMS_INLINE_ROUTINE Status_Control _MPCP_Surrender(
+        MPCP_Control         *mpcp,
+        Thread_Control       *executing,
+        Thread_queue_Context *queue_context
+)
+{
+    Thread_queue_Heads *heads;
+
+    if ( _MPCP_Get_owner( mpcp ) != executing ) {
+        _ISR_lock_ISR_enable( &queue_context->Lock_context.Lock_context );
+        return STATUS_NOT_OWNER;
+    }
+
+    _MPCP_Acquire_critical( mpcp, queue_context );
+
+    _MPCP_Set_owner( mpcp, NULL );
+    _MPCP_Remove_priority( executing, &mpcp->Ceiling_priority, queue_context );
+
+    heads = mpcp->Wait_queue.Queue.heads;
+
+    if ( heads == NULL ) {
+        Per_CPU_Control *cpu_self;
+
+        cpu_self = _Thread_Dispatch_disable_critical(
+                     &queue_context->Lock_context.Lock_context
+        );
+        _MPCP_Release( mpcp, queue_context );
+        _Thread_Priority_update( queue_context );
+        _Thread_Dispatch_enable( cpu_self );
+        return STATUS_SUCCESSFUL;
+    }
+
+    _Thread_queue_Surrender(
+            &mpcp->Wait_queue.Queue,
+            heads,
+            executing,
+            queue_context,
+            MPCP_TQ_OPERATIONS
+    );
+    return STATUS_SUCCESSFUL;
+}
+
+/**
+ * @brief Checks if the MPCP control can be destroyed.
+ *
+ * @param mpcp The MPCP control for the operation.
+ *
+ * @retval STATUS_SUCCESSFUL The MPCP is currently not used
+ *      and can be destroyed.
+ * @retval STATUS_RESOURCE_IN_USE The MPCP control is in use,
+ *      it cannot be destroyed.
+ */
+RTEMS_INLINE_ROUTINE Status_Control _MPCP_Can_destroy( MPCP_Control *mpcp )
+{
+    if ( _MPCP_Get_owner( mpcp ) != NULL ) {
+        return STATUS_RESOURCE_IN_USE;
+    }
+
+    return STATUS_SUCCESSFUL;
+}
+
+/**
+ * @brief Destroys the MPCP control
+ *
+ * @param[in, out] The mpcp that is about to be destroyed.
+ * @param queue_context The thread queue context.
+ */
+RTEMS_INLINE_ROUTINE void _MPCP_Destroy(
+        MPCP_Control         *mpcp,
+        Thread_queue_Context *queue_context
+)
+{
+    _MPCP_Release( mpcp, queue_context );
+    _Thread_queue_Destroy( &mpcp->Wait_queue );
+    _Workspace_Free( mpcp->ceiling_priorities );
+}
+
+/** @} */
+
+#ifdef __cplusplus
+}
+#endif /* __cplusplus */
+
+#endif /* RTEMS_SMP */
+
+#endif /* _RTEMS_SCORE_MPCPIMPL_H */
\ No newline at end of file
diff --git a/cpukit/include/rtems/score/schedulerimpl.h b/cpukit/include/rtems/score/schedulerimpl.h
index e7fbb8b166..e8c7afee53 100644
--- a/cpukit/include/rtems/score/schedulerimpl.h
+++ b/cpukit/include/rtems/score/schedulerimpl.h
@@ -1342,6 +1342,149 @@ RTEMS_INLINE_ROUTINE Status_Control _Scheduler_Set(
   return STATUS_SUCCESSFUL;
 }
 
+RTEMS_INLINE_ROUTINE void _Scheduler_Migrate_To(
+  Thread_Control  *executing,
+  Per_CPU_Control *migration_cpu,
+  Priority_Node   *ceiling_priority
+)
+{
+  ISR_lock_Context         lock_context;
+  const Scheduler_Control *migration_scheduler;
+  const Scheduler_Control *home_scheduler;
+  Scheduler_Node 	  *home_node;
+  Scheduler_Node 	  *migration_node;
+  size_t          	   migration_scheduler_index;
+  ISR_lock_Context         scheduler_lock_context;
+  Per_CPU_Control         *cpu_self;
+
+  home_node = _Thread_Scheduler_get_home_node( executing );
+  home_scheduler = _Thread_Scheduler_get_home( executing );
+
+  _Scheduler_Acquire_critical( home_scheduler, &lock_context );
+  ( *home_scheduler->Operations.block )(
+    home_scheduler,
+    executing,
+    home_node
+  );
+  _Scheduler_Release_critical( home_scheduler, &lock_context );
+
+  migration_scheduler = _Scheduler_Get_by_CPU(migration_cpu);
+  migration_scheduler_index = _Scheduler_Get_index(migration_scheduler);
+  migration_node = _Thread_Scheduler_get_node_by_index(executing ,migration_scheduler_index);
+
+  _Thread_Scheduler_acquire_critical( executing, &scheduler_lock_context );
+  _Scheduler_Node_set_priority( migration_node, ceiling_priority->priority, false );
+  _Chain_Append_unprotected(
+    &executing->Scheduler.Scheduler_nodes,
+    &migration_node->Thread.Scheduler_node.Chain
+  );
+  _Chain_Append_unprotected(
+    &executing->Scheduler.Wait_nodes,
+    &migration_node->Thread.Wait_node
+  );
+  _Thread_Scheduler_release_critical( executing, &scheduler_lock_context );
+
+  _Scheduler_Acquire_critical( migration_scheduler, &lock_context );
+  ( *migration_scheduler->Operations.update_priority )(
+    migration_scheduler,
+    executing,
+    migration_node
+  );
+  ( *migration_scheduler->Operations.unblock )(
+    migration_scheduler,
+    executing,
+    migration_node
+  );
+  _Scheduler_Release_critical( migration_scheduler, &lock_context );
+}
+
+RTEMS_INLINE_ROUTINE void _Scheduler_Migrate_Back(
+  Thread_Control  *executing,
+  Per_CPU_Control *migration_cpu
+)
+{
+  ISR_lock_Context         lock_context;
+  ISR_lock_Context 	   scheduler_lock_context;
+  const Scheduler_Control *migration_scheduler;
+  const Scheduler_Control *home_scheduler;
+  Scheduler_Node 	  *home_node;
+  Scheduler_Node 	  *migration_node;
+  size_t 		   migration_scheduler_index;
+
+  migration_scheduler = _Scheduler_Get_by_CPU( migration_cpu );
+  migration_scheduler_index = _Scheduler_Get_index( migration_scheduler );
+  migration_node = _Thread_Scheduler_get_node_by_index(
+    executing,
+    migration_scheduler_index
+  );
+
+  _Scheduler_Acquire_critical( migration_scheduler, &lock_context );
+  _Thread_Scheduler_acquire_critical( executing, &scheduler_lock_context );
+  _Scheduler_Node_set_priority(
+    migration_node,
+    migration_scheduler->maximum_priority,
+    false
+  );
+  _Thread_Scheduler_release_critical( executing, &scheduler_lock_context );
+  ( *migration_scheduler->Operations.update_priority )(
+    migration_scheduler,
+    executing,
+    migration_node
+  );
+  ( *migration_scheduler->Operations.block )(
+    migration_scheduler,
+    executing,
+    migration_node
+  );
+  _Scheduler_Release_critical( migration_scheduler, &lock_context );
+
+  _Thread_Scheduler_acquire_critical( executing, &scheduler_lock_context );
+  _Chain_Extract_unprotected( &migration_node->Thread.Wait_node );
+  _Chain_Extract_unprotected( &migration_node->Thread.Scheduler_node.Chain );
+  _Thread_Scheduler_release_critical( executing, &scheduler_lock_context );
+  home_node = _Thread_Scheduler_get_home_node( executing );
+  home_scheduler = _Thread_Scheduler_get_home( executing );
+
+  _Scheduler_Acquire_critical( home_scheduler, &lock_context );
+  ( *home_scheduler->Operations.unblock )(
+    home_scheduler,
+    executing,
+    home_node
+  );
+  _Scheduler_Release_critical( home_scheduler, &lock_context );
+}
+
+RTEMS_INLINE_ROUTINE void _Scheduler_Change_migration_priority(
+  Thread_Control  *executing,
+  Per_CPU_Control *migration_cpu,
+  Priority_Node   *priority
+)
+{
+  const Scheduler_Control *migration_scheduler;
+  Scheduler_Node  	  *migration_node;
+  size_t  		   migration_scheduler_index;
+  ISR_lock_Context    	   scheduler_lock_context;
+  ISR_lock_Context         lock_context;
+
+  migration_scheduler = _Scheduler_Get_by_CPU( migration_cpu );
+  migration_scheduler_index = _Scheduler_Get_index( migration_scheduler );
+  migration_node = _Thread_Scheduler_get_node_by_index(
+   executing,
+   migration_scheduler_index
+  );
+
+  _Scheduler_Acquire_critical( migration_scheduler, &lock_context );
+  _Thread_Scheduler_acquire_critical( executing, &scheduler_lock_context );
+  _Scheduler_Node_set_priority( migration_node, priority->priority, false );
+  _Thread_Scheduler_release_critical( executing, &scheduler_lock_context );
+  ( *migration_scheduler->Operations.update_priority )(
+    migration_scheduler,
+    executing,
+    migration_node
+  );
+  _Scheduler_Release_critical( migration_scheduler, &lock_context );
+}
+
 /** @} */
 
 #ifdef __cplusplus
diff --git a/cpukit/include/rtems/score/threadqimpl.h b/cpukit/include/rtems/score/threadqimpl.h
index 24a2ba5e60..bacc10bc67 100644
--- a/cpukit/include/rtems/score/threadqimpl.h
+++ b/cpukit/include/rtems/score/threadqimpl.h
@@ -930,6 +930,13 @@ void _Thread_queue_Enqueue(
   Thread_queue_Context          *queue_context
 );
 
+void _Thread_queue_Enqueue_busy(
+  Thread_queue_Queue            *queue,
+  const Thread_queue_Operations *operations,
+  Thread_Control                *the_thread,
+  Thread_queue_Context          *queue_context
+);
+
 #if defined(RTEMS_SMP)
 /**
  * @brief Enqueues the thread on the thread queue and busy waits for dequeue.
@@ -1111,7 +1118,6 @@ void _Thread_queue_Surrender(
   const Thread_queue_Operations *operations
 );
 
-#if defined(RTEMS_SMP)
 /**
  * @brief Surrenders the thread queue previously owned by the thread to the
  * first enqueued thread.
diff --git a/cpukit/rtems/src/semcreate.c b/cpukit/rtems/src/semcreate.c
index ea89800685..625b066c1e 100644
--- a/cpukit/rtems/src/semcreate.c
+++ b/cpukit/rtems/src/semcreate.c
@@ -29,7 +29,8 @@
 #include <rtems/sysinit.h>
 
 #define SEMAPHORE_KIND_MASK ( RTEMS_SEMAPHORE_CLASS | RTEMS_INHERIT_PRIORITY \
-  | RTEMS_PRIORITY_CEILING | RTEMS_MULTIPROCESSOR_RESOURCE_SHARING )
+  | RTEMS_PRIORITY_CEILING | RTEMS_MULTIPROCESSOR_RESOURCE_SHARING \
+  | RTEMS_DISTRIBUTED_PRIORITY_CEILING | RTEMS_MULTIPROCESSOR_PRIORITY_CEILING)
 
 rtems_status_code rtems_semaphore_create(
   rtems_name           name,
@@ -109,9 +110,34 @@ rtems_status_code rtems_semaphore_create(
      */
     variant = SEMAPHORE_VARIANT_MUTEX_PRIORITY_CEILING;
 #endif
-  } else {
+  } else if (
+      mutex_with_protocol
+	== ( RTEMS_BINARY_SEMAPHORE | RTEMS_DISTRIBUTED_PRIORITY_CEILING |  \
+	    RTEMS_GLOBAL)
+    ) {
+    #if defined(RTEMS_SMP)
+        variant = SEMAPHORE_VARIANT_DPCP;
+    #else
+        /*
+        * Use normal PCP on uni-processor
+        */
+        variant = SEMAPHORE_VARIANT_MUTEX_PRIORITY_CEILING;
+    #endif
+  }else if (
+    mutex_with_protocol
+      == ( RTEMS_BINARY_SEMAPHORE | RTEMS_MULTIPROCESSOR_PRIORITY_CEILING)
+        ) {
+    #if defined(RTEMS_SMP)
+	    variant = SEMAPHORE_VARIANT_MPCP;
+    #else
+      /*
+      * Use normal PCP on uni-processor
+      */
+      variant = RTEMS_MP_NOT_CONFIGURED;
+    #endif
+  }else {
     return RTEMS_NOT_DEFINED;
-  }
+}
 
   the_semaphore = _Semaphore_Allocate();
 
@@ -224,6 +250,40 @@ rtems_status_code rtems_semaphore_create(
         status = STATUS_INVALID_PRIORITY;
       }
 
+      break;
+    case SEMAPHORE_VARIANT_DPCP:
+      scheduler = _Thread_Scheduler_get_home( executing );
+      priority = _RTEMS_Priority_To_core( scheduler, priority_ceiling, &valid );
+
+      if ( valid ) {
+        status = _DPCP_Initialize(
+          &the_semaphore->Core_control.DPCP,
+          scheduler,
+          priority,
+          executing,
+          count == 0
+        );
+      } else {
+        status = STATUS_INVALID_PRIORITY;
+      }
+
+      break;
+    case SEMAPHORE_VARIANT_MPCP:
+      scheduler = _Thread_Scheduler_get_home( executing );
+      priority = _RTEMS_Priority_To_core( scheduler, priority_ceiling, &valid );
+
+      if ( valid ) {
+        status = _MPCP_Initialize(
+	 &the_semaphore->Core_control.MPCP,
+	 scheduler,
+	 priority,
+	 executing,
+	 count == 0
+       );
+      } else {
+        status = STATUS_INVALID_PRIORITY;
+      }
+
       break;
 #endif
     default:
diff --git a/cpukit/rtems/src/semdelete.c b/cpukit/rtems/src/semdelete.c
index b4564fbf25..c23280c082 100644
--- a/cpukit/rtems/src/semdelete.c
+++ b/cpukit/rtems/src/semdelete.c
@@ -72,6 +72,14 @@ rtems_status_code rtems_semaphore_delete(
     case SEMAPHORE_VARIANT_MRSP:
       status = _MRSP_Can_destroy( &the_semaphore->Core_control.MRSP );
       break;
+
+    case SEMAPHORE_VARIANT_DPCP:
+      status = _DPCP_Can_destroy( &the_semaphore->Core_control.DPCP );
+      break;
+
+    case SEMAPHORE_VARIANT_MPCP:
+      status = _MPCP_Can_destroy( &the_semaphore->Core_control.MPCP);
+      break;
 #endif
     default:
       _Assert(
@@ -98,6 +106,14 @@ rtems_status_code rtems_semaphore_delete(
     case SEMAPHORE_VARIANT_MRSP:
       _MRSP_Destroy( &the_semaphore->Core_control.MRSP, &queue_context );
       break;
+
+    case SEMAPHORE_VARIANT_DPCP:
+      _DPCP_Destroy( &the_semaphore->Core_control.DPCP, &queue_context );
+      break;
+
+    case SEMAPHORE_VARIANT_MPCP:
+      _MPCP_Destroy( &the_semaphore->Core_control.MPCP, &queue_context );
+      break;
 #endif
     default:
       _Assert(
diff --git a/cpukit/rtems/src/semflush.c b/cpukit/rtems/src/semflush.c
index a657a71366..3dfba3f63a 100644
--- a/cpukit/rtems/src/semflush.c
+++ b/cpukit/rtems/src/semflush.c
@@ -58,6 +58,18 @@ rtems_status_code rtems_semaphore_flush( rtems_id id )
         &queue_context
       );
       return RTEMS_NOT_DEFINED;
+    case SEMAPHORE_VARIANT_DPCP:
+      _Thread_queue_Release(
+        &the_semaphore->Core_control.Wait_queue,
+        &queue_context
+      );
+      return RTEMS_NOT_DEFINED;
+    case SEMAPHORE_VARIANT_MPCP:
+      _Thread_queue_Release(
+        &the_semaphore->Core_control.Wait_queue,
+        &queue_context
+      );
+      return RTEMS_NOT_DEFINED;
 #endif
     default:
       _Assert(
diff --git a/cpukit/rtems/src/semobtain.c b/cpukit/rtems/src/semobtain.c
index ae1cb864b7..cc157cb3a7 100644
--- a/cpukit/rtems/src/semobtain.c
+++ b/cpukit/rtems/src/semobtain.c
@@ -46,6 +46,18 @@ THREAD_QUEUE_OBJECT_ASSERT(
   Core_control.MRSP.Wait_queue,
   SEMAPHORE_CONTROL_MRSP
 );
+
+THREAD_QUEUE_OBJECT_ASSERT(
+  Semaphore_Control,
+  Core_control.MRSP.Wait_queue,
+  SEMAPHORE_CONTROL_DPCP
+);
+
+THREAD_QUEUE_OBJECT_ASSERT(
+  Semaphore_Control,
+  Core_control.MPCP.Wait_queue,
+  SEMAPHORE_CONTROL_MPCP
+);
 #endif
 
 rtems_status_code rtems_semaphore_obtain(
@@ -123,6 +135,22 @@ rtems_status_code rtems_semaphore_obtain(
         &queue_context
       );
       break;
+    case SEMAPHORE_VARIANT_DPCP:
+      status = _DPCP_Seize(
+        &the_semaphore->Core_control.DPCP,
+        executing,
+        wait,
+        &queue_context
+      );
+      break;
+    case SEMAPHORE_VARIANT_MPCP:
+      status = _MPCP_Seize(
+        &the_semaphore->Core_control.MPCP,
+        executing,
+        wait,
+        &queue_context
+      );
+      break;
 #endif
     default:
       _Assert(
diff --git a/cpukit/rtems/src/semrelease.c b/cpukit/rtems/src/semrelease.c
index e5b0e4834d..45bb16e1da 100644
--- a/cpukit/rtems/src/semrelease.c
+++ b/cpukit/rtems/src/semrelease.c
@@ -98,6 +98,20 @@ rtems_status_code rtems_semaphore_release( rtems_id id )
         &queue_context
       );
       break;
+    case SEMAPHORE_VARIANT_DPCP:
+      status = _DPCP_Surrender(
+        &the_semaphore->Core_control.DPCP,
+        executing,
+        &queue_context
+      );
+      break;
+    case SEMAPHORE_VARIANT_MPCP:
+      status = _MPCP_Surrender(
+        &the_semaphore->Core_control.MPCP,
+        executing,
+        &queue_context
+      );
+      break;
 #endif
     default:
       _Assert( variant == SEMAPHORE_VARIANT_COUNTING );
diff --git a/cpukit/rtems/src/semsetpriority.c b/cpukit/rtems/src/semsetpriority.c
index 933114f96a..bcfeaf4b86 100644
--- a/cpukit/rtems/src/semsetpriority.c
+++ b/cpukit/rtems/src/semsetpriority.c
@@ -97,6 +97,32 @@ static rtems_status_code _Semaphore_Set_priority(
         );
       }
 
+      sc = RTEMS_SUCCESSFUL;
+      break;
+    case SEMAPHORE_VARIANT_MPCP:
+      if ( new_priority != RTEMS_CURRENT_PRIORITY ) {
+         _MPCP_Set_priority(
+           &the_semaphore->Core_control.MPCP,
+           scheduler,
+           core_priority
+         );
+       }
+
+      sc = RTEMS_SUCCESSFUL;
+      break;
+    case SEMAPHORE_VARIANT_DPCP:
+      old_priority = _DPCP_Get_priority(
+        &the_semaphore->Core_control.DPCP
+      );
+
+      if ( new_priority != RTEMS_CURRENT_PRIORITY ) {
+        _DPCP_Set_priority(
+          &the_semaphore->Core_control.DPCP,
+          scheduler,
+          core_priority
+        );
+      }
+
       sc = RTEMS_SUCCESSFUL;
       break;
 #endif
diff --git a/cpukit/rtems/src/semsetprocessor.c b/cpukit/rtems/src/semsetprocessor.c
new file mode 100644
index 0000000000..3924ce944b
--- /dev/null
+++ b/cpukit/rtems/src/semsetprocessor.c
@@ -0,0 +1,90 @@
+/**
+ * @file
+ *
+ * @brief RTEMS Semaphore Release
+ * @ingroup ClassicSem Semaphores
+ *
+ * This file contains the implementation of the Classic API directive
+ * rtems_semaphore_release().
+ */
+
+/*
+ *  COPYRIGHT (c) 1989-2014.
+ *  On-Line Applications Research Corporation (OAR).
+ *
+ *  The license and distribution terms for this file may be
+ *  found in the file LICENSE in this distribution or at
+ *  http://www.rtems.org/license/LICENSE.
+ */
+
+#if HAVE_CONFIG_H
+#include "config.h"
+#endif
+
+#include <rtems/rtems/semimpl.h>
+#include <rtems/rtems/statusimpl.h>
+
+rtems_status_code rtems_semaphore_set_processor(
+  rtems_id id,
+  int      cpu
+)
+{
+  Semaphore_Control   *the_semaphore;
+  Thread_queue_Context queue_context;
+  ISR_lock_Context     lock_context;
+  uintptr_t            flags;
+  Semaphore_Variant    variant;
+  Status_Control       status;
+  
+
+  the_semaphore = _Semaphore_Get( id, &queue_context );
+
+  if ( the_semaphore == NULL ) {
+#if defined(RTEMS_MULTIPROCESSING)
+    return _Semaphore_MP_Release( id );
+#else
+    return RTEMS_INVALID_ID;
+#endif
+  }
+
+  _Thread_queue_Context_set_MP_callout(
+    &queue_context,
+    _Semaphore_Core_mutex_mp_support
+  );
+  flags = _Semaphore_Get_flags( the_semaphore );
+  variant = _Semaphore_Get_variant( flags );
+
+  switch ( variant ) {
+    case SEMAPHORE_VARIANT_MUTEX_INHERIT_PRIORITY:
+      status =  RTEMS_NOT_DEFINED;
+      break;
+    case SEMAPHORE_VARIANT_MUTEX_PRIORITY_CEILING:
+      status = RTEMS_NOT_DEFINED;
+      break;
+    case SEMAPHORE_VARIANT_MUTEX_NO_PROTOCOL:
+      status = RTEMS_NOT_DEFINED;
+      break;
+    case SEMAPHORE_VARIANT_SIMPLE_BINARY:
+      status = RTEMS_NOT_DEFINED;
+      break;
+#if defined(RTEMS_SMP)
+    case SEMAPHORE_VARIANT_MRSP:
+      status = RTEMS_NOT_DEFINED;
+      break;
+    case SEMAPHORE_VARIANT_DPCP:
+     _DPCP_Set_CPU(
+       &the_semaphore->Core_control.DPCP,
+       _Per_CPU_Get_by_index(cpu),
+       &queue_context
+     );
+     status = STATUS_SUCCESSFUL;
+     break;
+#endif
+    default:
+      _Assert(variant == SEMAPHORE_VARIANT_COUNTING );
+      status = RTEMS_NOT_DEFINED;
+      break;
+  }
+
+  return _Status_Get( status );
+}
diff --git a/cpukit/score/src/threadqenqueue.c b/cpukit/score/src/threadqenqueue.c
index 972736f3f2..ea3356e7b2 100644
--- a/cpukit/score/src/threadqenqueue.c
+++ b/cpukit/score/src/threadqenqueue.c
@@ -26,6 +26,7 @@
 #include <rtems/score/threaddispatch.h>
 #include <rtems/score/threadimpl.h>
 #include <rtems/score/status.h>
+#include <rtems/score/schedulerimpl.h>
 #include <rtems/score/watchdogimpl.h>
 
 #define THREAD_QUEUE_INTEND_TO_BLOCK \
@@ -452,6 +453,63 @@ void _Thread_queue_Enqueue(
   _Thread_Dispatch_direct( cpu_self );
 }
 
+void _Thread_queue_Enqueue_busy(
+  Thread_queue_Queue            *queue,
+  const Thread_queue_Operations *operations,
+  Thread_Control                *the_thread,
+  Thread_queue_Context          *queue_context
+)
+{
+  Per_CPU_Control *cpu_self;
+
+  _Assert( queue_context->enqueue_callout != NULL );
+
+#if defined(RTEMS_MULTIPROCESSING)
+  if ( _Thread_MP_Is_receive( the_thread ) && the_thread->receive_packet ) {
+    the_thread = _Thread_MP_Allocate_proxy( queue_context->thread_state );
+  }
+#endif
+
+  _Thread_Wait_claim( the_thread, queue );
+
+  if ( !_Thread_queue_Path_acquire_critical( queue, the_thread, queue_context ) ) {
+    _Thread_queue_Path_release_critical( queue_context );
+    _Thread_Wait_restore_default( the_thread );
+    _Thread_queue_Queue_release( queue, &queue_context->Lock_context.Lock_context );
+    _Thread_Wait_tranquilize( the_thread );
+    _Assert( queue_context->deadlock_callout != NULL );
+    ( *queue_context->deadlock_callout )( the_thread );
+    return;
+  }
+
+  _Thread_queue_Context_clear_priority_updates( queue_context );
+  _Thread_Wait_claim_finalize( the_thread, operations );
+  ( *operations->enqueue )( queue, the_thread, queue_context );
+
+  _Thread_queue_Path_release_critical( queue_context );
+
+  the_thread->Wait.return_code = STATUS_SUCCESSFUL;
+  _Thread_Wait_flags_set( the_thread, THREAD_QUEUE_INTEND_TO_BLOCK );
+  _Thread_queue_Queue_release( queue, &queue_context->Lock_context.Lock_context );
+
+  ( *queue_context->enqueue_callout )(
+    queue,
+    the_thread,
+    cpu_self,
+    queue_context
+  );
+
+  while (
+     _Thread_Wait_flags_get_acquire( the_thread ) == THREAD_QUEUE_INTEND_TO_BLOCK
+   ) {
+     /* Wait */
+   }
+
+
+  _Thread_Timer_remove( the_thread );
+  _Thread_Dispatch_direct( cpu_self );
+}
+
 #if defined(RTEMS_SMP)
 Status_Control _Thread_queue_Enqueue_sticky(
   Thread_queue_Queue            *queue,
diff --git a/cpukit/score/src/threadqops.c b/cpukit/score/src/threadqops.c
index 8c3e0cb1dc..87171874e6 100644
--- a/cpukit/score/src/threadqops.c
+++ b/cpukit/score/src/threadqops.c
@@ -1459,6 +1459,14 @@ const Thread_queue_Operations _Thread_queue_Operations_default = {
    */
 };
 
+const Thread_queue_Operations _Thread_queue_Operations_FIFO_PIP = {
+  .priority_actions = _Thread_queue_Priority_inherit_priority_actions,
+  .enqueue = _Thread_queue_FIFO_enqueue,
+  .extract = _Thread_queue_FIFO_extract,
+  .surrender = _Thread_queue_FIFO_surrender,
+  .first = _Thread_queue_FIFO_first
+};
+
 const Thread_queue_Operations _Thread_queue_Operations_FIFO = {
   .priority_actions = _Thread_queue_Do_nothing_priority_actions,
   .enqueue = _Thread_queue_FIFO_enqueue,
